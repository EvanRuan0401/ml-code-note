{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 感知機 (Perceptron)\n",
    "感知機是一種線性二元分類模型，它由一個線性函數與一個激活函數組成，其模型表達式如下：\n",
    "\n",
    "$$\n",
    "y_i=f(X_i)=\\operatorname{sign}(W \\cdot X_i+b)\n",
    "$$\n",
    "\n",
    "其中，$X_i=(x_{i1}, x_{i2}, ......, x_{ik})^T$，每個樣本有k個特徵項；整體模型待估計的參數有k+1個，包含$W=(w_{1}, w_{2}, ......, w_{k})$以及$b$，且\n",
    "\n",
    "\n",
    "$$\n",
    "\\operatorname{sign}(z)=\\left\\{\\begin{array}{ll}{+1,} & {z \\geqslant 0} \\\\ {-1,} & {z<0}\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "感知機的核心精神是在k維的特徵空間中找出能將數據分為兩類的「分離超平面」，模型中的$W \\cdot X_i+b=0$即為「分離超平面」，若數據落在超平面右邊記為正類（即$W \\cdot X_i+b\\geqslant0$），\n",
    "相反地，若數據落在超平面左邊則記為負類（即$W \\cdot X_i+b<0$）\n",
    "\n",
    "題外話：感知機僅能在數據線性可分時實現，而神經網路就是透過多個感知機來實現單個感知機無法實現的分類問題。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 模型損失函數\n",
    "首先，必須先決定感知機的損失函數，在二元分類問題中，常用的損失函數為準確率(accuracy)、準確率(precision)以及召回率(recall)，不過，在感知機的框架下，模型的目的是找出分離超平面，換句話說就是找出能將數據分成兩類的($W$,$b$)，而上述三者並無法表達為($W$,$b$)的連續可微函數，因此，在感知機中是透過極小化「每個錯誤分類樣本」在特徵空間中與超平面的距離總和來求得參數($W$,$b$)，由於正確分類樣本已經被分類正確，將其加入損失函數並去計算其與超平面距離並沒有什麼意義，因此損失函數與距離函數只需要針對那些被錯誤分類的樣本；我們可以進一步探討錯誤分類的可能性有以下兩種：\n",
    "\n",
    "1.真實的$X_i$為正類（$y_i=+1$），但卻被分到負類，誤以負類的距離函數計算它 <p>\n",
    "    距離=$\\frac{-W \\cdot X_i + b}{\\lVert W\\rVert}$<p/>\n",
    "2.真實的$X_i$為負類（$y_i=-1$），但卻被分到正類，誤以正類的距離函數計算它 <p>\n",
    "    距離=$\\frac{W \\cdot X_i + b}{\\lVert W\\rVert}$<p/>\n",
    "其中，$\\lVert W\\rVert=\\sqrt{(w_1^2+w_2^2+\\cdots +w_k^2)}$，整理上述兩種可能之距離函數，可得到被錯誤分類的樣本$X_i$與超平面之間的距離為：\n",
    "$$\n",
    "\\frac{-y_i(W\\cdot X_i + b)}{\\lVert W\\rVert}\n",
    "$$\n",
    "接著，我們假設錯誤分類樣本的集合為$M$，將所有錯誤分類樣本到超平面的距離加總，並極小化該式，可得：\n",
    "\n",
    "$$\n",
    "\\min _{W, b} \\frac{-1}{\\lVert W\\rVert}\\sum_{X_{i} \\in M} y_{i}\\left(W \\cdot X_{i}+b\\right)\n",
    "$$\n",
    "\n",
    "而$\\lVert W\\rVert$並不影響感知機最終的結果，可以直觀理解為$\\lVert W\\rVert$之計算結果不影響該式的正負值，且模型學習的最終結果為不存在錯誤分類樣本，因此整個極小化式子的最終結果必須為0，在此情況下，范數$\\lVert W\\rVert$為多少都不影響最終結果；因此，我們可以再進一步簡化損失函數為：\n",
    "\n",
    "$$\n",
    "\\min _{W, b} L(w, b)=-\\sum_{x_{i} \\in M} y_{i}\\left(W \\cdot x_{i}+b\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "# 學習算法\n",
    "感知機的學習算法是採用Stochastic Gradient Decent，它是在Gradient Decent的過程中，在每次更新參數時只隨機地選一個錯誤分類的樣本來計算梯度，目的是讓超平面向該錯誤分類樣本的一側移動調整，直到該樣本被正確分類，過程中不斷地使損失函數減少，最終減至0；根據感知機損失函數，參數$(W,b)$的梯度可以以下兩式表示：\n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla_W L(W, b)=-\\sum_{x_{i} \\in M}y_iX_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_b L(W, b)=-\\sum_{x_{i} \\in M}y_i\n",
    "$$\n",
    "\n",
    "不過由於在此學習算法是採用Stochastic Gradient Decent，因此可將上兩式改寫為：\n",
    "\n",
    "$$\n",
    "\\nabla_W L(W, b)=-y_iX_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_b L(W, b)=-y_i\n",
    "$$\n",
    "\n",
    "加入參數更新過程公式並整理可得模型參數$(W,b)$的更新過程：\n",
    "\n",
    "$$\n",
    "W^{t+1}=W^t + \\eta y_iX_i\n",
    "$$\n",
    "$$\n",
    "b^{t+1}=b^t + \\eta y_i\n",
    "$$\n",
    "\n",
    " \n",
    "# 補充 \n",
    "1. 當訓練數據線性可分時，感知機學習算法是收斂的，錯誤分類的次數$m$將會有個上限次數。\n",
    "\n",
    "\n",
    "2. 當訓練數據線性可分時，感知機學習算法存在無限多個解，解的不同的來自於初使值$(W_0,b_0)$設定的不同或不同迭代更新參數的順序而不同。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
