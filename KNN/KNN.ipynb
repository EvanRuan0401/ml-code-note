{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-NN近鄰法\n",
    "K-NN是一種機器學習方法，可以應用於迴歸問題與分類問題，不過，其更多應用於分類問題，可以分類$m$個類別數，K-NN是一個沒有顯示學習過程的方法，換句話說，它沒有一個函數或是機率分布來表達自變數與應變數之間的關係；其核心為在給定的訓練樣本集$T=\\{(X_{1}, y_{1}),(X_{2}, y_{2}), \\cdots,(X_{n}, y_{n})\\}$，且$y_i \\in \\{c_1,c_2,\\cdots, c_m\\}$有m個分類類別的情況下，輸入測試樣本$X$（或預測樣本），找出$k$個距離該測試樣本最近的訓練樣本，以$N_k(X)$表示那k個近鄰點的範圍，預測樣本之預測分類$y$為範圍$N_k(X)$中佔最多數的類別，如下式表達：\n",
    "\n",
    "$$\n",
    "y = \\max_{c_j} \\sum_{X_{i} \\in N_k(X)} I(y_i=c_j)\n",
    "$$  \n",
    "$$ i=1,2,\\cdots, n;j=1,2,\\cdots,m$$\n",
    "\n",
    "註：若$k$太小可能導致overfitting的機率較高，而若$k$太大，則可能使訓練誤差較大。$k$值的選擇反映了對訓練誤差與預測誤差之間的權衡，一般通常由交叉驗證選擇在測試集上表現最優的$k$。\n",
    "\n",
    "# 距離度量\n",
    "\n",
    "為了找出$k$個近鄰，需透過距離函數來計算得到，而關於衡量樣本點之間距離的方式有很多種，在此以$L_p$距離表示；假設特徵空間$X$是$n$維實數向量空間 ，$x_{i}, x_{j} \\in \\mathcal{X}$,$x_{i}=\\left(x_{i}^{(1)}, x_{i}^{(2)}, \\cdots, x_{i}^{(n)}\\right)^{\\mathrm{T}}$,$x_{j}=\\left(x_{j}^{(1)}, x_{j}^{(2)}, \\cdots, x_{j}^{(n)}\\right)^{\\mathrm{T}}$\n",
    "，則：$x_i$,$x_j$的$L_p$距離定義為:\n",
    "\n",
    "\n",
    "$L_{p}\\left(x_{i}, x_{j}\\right)=\\left(\\sum_{i=1}^{n}\\left|x_{i}^{(i)}-x_{j}^{(l)}\\right|^{p}\\right)^{\\frac{1}{p}}$\n",
    "\n",
    "- $p= 1$  曼哈頓距離\n",
    "- $p= 2$  歐氏距離\n",
    "- $p= \\infty$   柴比雪夫距離\n",
    "\n",
    "註：\n",
    "1. 採用不同度量所得到的鄰近點可能會不同\n",
    "2. 一般而言，在計算距離前，會事先將每個特徵進行Normalize，避免特徵之間由於單位、數據大小不同而導致距離的計算較無意義。  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
